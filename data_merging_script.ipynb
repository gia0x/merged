{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Viewing RDS (OPTIONAL)"
      ],
      "metadata": {
        "id": "BJMX3CMJ2YzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pyreadr\n",
        "import glob\n",
        "import pandas as pd\n",
        "import pyreadr"
      ],
      "metadata": {
        "id": "Yrf2Aqnr2eeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rds_path = glob.glob(\"/content/*.rds\")[0]\n",
        "print(\"Reading:\", rds_path)\n",
        "\n",
        "res = pyreadr.read_r(rds_path)\n",
        "df = next(iter(res.values()))\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", 0)\n",
        "\n",
        "print(\"shape:\", df.shape)\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "rInQLeBfrqbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA STACKING"
      ],
      "metadata": {
        "id": "huBVcoWe3FOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import pyreadr\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "TRaMinfmttC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REPO_DIR = \"/content/\"\n",
        "# main csv that will store the collected data\n",
        "MASTER_OUTPUT = f\"{REPO_DIR}/standardized_master.csv\""
      ],
      "metadata": {
        "id": "iYGyrur6cgdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This assumes metadata.csv is in the current working directory.\n",
        "# If not, upload it once at the correct path.\n",
        "\n",
        "METADATA_NAME = \"/content/metadata.csv\"\n",
        "\n",
        "master_df = pd.read_csv(METADATA_NAME)\n",
        "master_df.columns = [c.strip() for c in master_df.columns]\n",
        "master_df = master_df.loc[:, ~master_df.columns.str.contains(r\"^Unnamed\")]\n",
        "master_columns = list(master_df.columns)\n",
        "print(\"Loaded metadata sheet\")\n",
        "print(\"Number of master columns:\", len(master_columns))\n",
        "# print(\"First few columns:\", master_columns[:10])"
      ],
      "metadata": {
        "id": "KrCgW-JazuCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIND DATA FILE - CSV VERSION\n",
        "import os\n",
        "\n",
        "csv_files = [\n",
        "    os.path.join(\"/content\", f)\n",
        "    for f in os.listdir(\"/content\")\n",
        "    if f.lower().endswith(\".csv\")\n",
        "]\n",
        "\n",
        "data_files = [\n",
        "    f for f in csv_files\n",
        "    if os.path.basename(f) != \"metadata.csv\"\n",
        "    and not os.path.basename(f).startswith(\"standardized_\")\n",
        "]\n",
        "\n",
        "print(\"CSV files found in /content:\", [os.path.basename(f) for f in csv_files])\n",
        "\n",
        "if not data_files:\n",
        "    raise RuntimeError(\"No data CSV found (only metadata.csv and/or standardized_*.csv). Drag a raw data file in.\")\n",
        "\n",
        "data_fn = data_files\n",
        "print(\"Using data file:\", data_fn)\n",
        "\n",
        "df_raw = pd.read_csv(\n",
        "    data_fn,\n",
        "    engine=\"python\",\n",
        "    on_bad_lines=\"warn\"\n",
        ")\n",
        "print(f\"\\n Rows: {len(df_raw)}\")\n",
        "print(f\" Columns ({len(df_raw.columns)}):\")\n",
        "print(list(df_raw.columns))"
      ],
      "metadata": {
        "id": "RTR5TyNwfQFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # FIND DATA FILE - RDS VERSION\n",
        "\n",
        "# import os\n",
        "\n",
        "# rds_files = [\n",
        "#     os.path.join(\"/content\", f)\n",
        "#     for f in os.listdir(\"/content\")\n",
        "#     if f.lower().endswith(\".rds\")\n",
        "# ]\n",
        "# print(\"RDS files found in /content:\", [os.path.basename(f) for f in rds_files])\n",
        "\n",
        "# data_files = [\n",
        "#     f for f in rds_files\n",
        "#     if os.path.basename(f) != \"metadata.  csv\"\n",
        "#     and not os.path.basename(f).startswith(\"standardized_\")\n",
        "# ]\n",
        "\n",
        "# if not data_files:\n",
        "#     raise RuntimeError(\"No data RDS found (only metadata.csv and/or standardized_*.rds). Upload a raw .rds file in /content.\")\n",
        "\n",
        "# data_fn = data_files\n",
        "# print(\"Using data file:\", data_fn)\n",
        "\n",
        "# result = pyreadr.read_r(data_fn)\n",
        "# print(\"Objects inside RDS:\", list(result.keys()))\n",
        "# df_raw = next(iter(result.values()))\n",
        "\n",
        "# print(f\"\\n Rows: {len(df_raw)}\")\n",
        "# print(f\" Columns ({len(df_raw.columns)}):\")\n",
        "# print(list(df_raw.columns))"
      ],
      "metadata": {
        "id": "5s6G6OCk7ES6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_name(name: str) -> str:\n",
        "    if name is None:\n",
        "        return \"\"\n",
        "    s = str(name).strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.lower()"
      ],
      "metadata": {
        "id": "umdX5Y1zz4ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_df(df_raw: pd.DataFrame, master_cols: list, meta: dict) -> pd.DataFrame:\n",
        "    raw_cols = list(df_raw.columns)\n",
        "    norm_raw_map = {normalize_name(c): c for c in raw_cols}\n",
        "    norm_master_map = {normalize_name(c): c for c in master_cols}\n",
        "\n",
        "    out_df = pd.DataFrame(index=df_raw.index, columns=master_cols)\n",
        "\n",
        "    for col, value in meta.items():\n",
        "        if col in out_df.columns:\n",
        "            out_df[col] = value\n",
        "        else:\n",
        "            print(f\"Meta column '{col}' not in master, skipping.\")\n",
        "\n",
        "    unmatched_raw = []\n",
        "    matched_pairs = []\n",
        "\n",
        "    for norm_raw, raw_col in norm_raw_map.items():\n",
        "        if norm_raw in norm_master_map:\n",
        "            master_col = norm_master_map[norm_raw]\n",
        "            out_df[master_col] = df_raw[raw_col]\n",
        "            matched_pairs.append((raw_col, master_col))\n",
        "        else:\n",
        "            unmatched_raw.append(raw_col)\n",
        "\n",
        "    print(\"Matched columns (raw â†’ master):\")\n",
        "    for r, m in matched_pairs:\n",
        "        print(f\"  {r} â†’ {m}\")\n",
        "\n",
        "    if unmatched_raw:\n",
        "        print(\"\\nRaw columns with NO match in master header:\")\n",
        "        for c in unmatched_raw:\n",
        "            print(\" \", c)\n",
        "\n",
        "    return out_df"
      ],
      "metadata": {
        "id": "U84g9eQD_kPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_consistency_check(df_raw, df_std, n_samples=25):\n",
        "\n",
        "    raw_cols = list(df_raw.columns)\n",
        "    std_cols = list(df_std.columns)\n",
        "\n",
        "    norm_raw = {normalize_name(c): c for c in raw_cols}\n",
        "    norm_std = {normalize_name(c): c for c in std_cols}\n",
        "\n",
        "    common_norm = [c for c in norm_raw.keys() if c in norm_std.keys()]\n",
        "\n",
        "    print(f\"Raw columns: {len(raw_cols)}\")\n",
        "    print(f\"Std columns: {len(std_cols)}\")\n",
        "    print(f\"Overlapping columns to compare: {len(common_norm)}\")\n",
        "\n",
        "    if len(common_norm) == 0:\n",
        "        print(\"No overlapping columns to compare. Check your schema.\")\n",
        "        return\n",
        "\n",
        "    raw_overlap_cols = [norm_raw[n] for n in common_norm]\n",
        "    std_overlap_cols = [norm_std[n] for n in common_norm]\n",
        "\n",
        "    n = min(n_samples, len(df_raw))\n",
        "    sample_idx = np.random.choice(len(df_raw), size=n, replace=False)\n",
        "\n",
        "    mismatches_total = 0\n",
        "\n",
        "    for idx in sample_idx:\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"Row index {idx}\")\n",
        "\n",
        "        row_mismatches = 0\n",
        "\n",
        "        raw_row_overlap = df_raw.iloc[idx][raw_overlap_cols]\n",
        "        std_row_overlap = df_std.iloc[idx][std_overlap_cols]\n",
        "\n",
        "        print(\"\\nRAW (overlapping columns):\")\n",
        "        print(raw_row_overlap.to_dict())\n",
        "\n",
        "        print(\"\\nSTD (overlapping columns):\")\n",
        "        print(std_row_overlap.to_dict())\n",
        "\n",
        "        for norm_name in common_norm:\n",
        "            r_col = norm_raw[norm_name]\n",
        "            s_col = norm_std[norm_name]\n",
        "\n",
        "            v_raw = df_raw.iloc[idx][r_col]\n",
        "            v_std = df_std.iloc[idx][s_col]\n",
        "\n",
        "            both_nan = pd.isna(v_raw) and pd.isna(v_std)\n",
        "\n",
        "            if not both_nan and v_raw != v_std:\n",
        "                if row_mismatches == 0:\n",
        "                    print(\"\\n  Mismatches in this row:\")\n",
        "                print(f\"    {r_col} (raw) = {repr(v_raw)}  |  {s_col} (std) = {repr(v_std)}\")\n",
        "                row_mismatches += 1\n",
        "                mismatches_total += 1\n",
        "\n",
        "        if row_mismatches == 0:\n",
        "            print(\"\\n  All overlapping columns match for this row.\")\n",
        "\n",
        "    if mismatches_total == 0:\n",
        "        print(\"\\nAll sampled rows match perfectly on all overlapping columns.\")\n",
        "    else:\n",
        "        print(f\"\\nFound {mismatches_total} mismatches across {n} sampled rows.\")"
      ],
      "metadata": {
        "id": "NamcWlvPCwev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THIS LINE FOR EACH NEW CSV/ RDS FILE THATS ADDED TO THE MASTER\n",
        "metadata_line = 'covid_19\tsimulation\tpredictions\t\t\t\tols_prediction_results.rds'\n",
        "parts = metadata_line.strip().split(\"\\t\")\n",
        "\n",
        "if len(parts) != 7:\n",
        "    raise ValueError(f\"Expected 7 tab-separated values, got {len(parts)}: {parts}\")\n",
        "\n",
        "meta = {\n",
        "    \"Main folder\": parts[0],\n",
        "    \"Level 1\":     parts[1],\n",
        "    \"Level 2\":     parts[2],\n",
        "    \"Level 3\":     parts[3],\n",
        "    \"Level 4\":     parts[4],\n",
        "    \"Level 5\":     parts[5],\n",
        "    \"df_name\":     parts[6]\n",
        "}\n",
        "\n",
        "# --- check df_name vs actual file name ---\n",
        "provided_name = parts[6].strip()\n",
        "actual_name   = os.path.basename(data_fn).strip()\n",
        "\n",
        "if provided_name != actual_name:\n",
        "    raise ValueError(\n",
        "        f\"df_name from metadata ({provided_name}) does NOT match actual file name ({actual_name}).\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"df_name matches file name: {provided_name}\")\n",
        "\n",
        "print(\"Parsed meta:\")\n",
        "for k, v in meta.items():\n",
        "    print(f\"  {k}: {v}\")"
      ],
      "metadata": {
        "id": "iUHzCm8a1NFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standardized_df = standardize_df(df_raw, master_columns, meta)\n",
        "\n",
        "if os.path.exists(MASTER_OUTPUT):\n",
        "    existing = pd.read_csv(MASTER_OUTPUT, low_memory=False)\n",
        "    existing.columns = [c.strip() for c in existing.columns]\n",
        "\n",
        "    if \"df_name\" in existing.columns:\n",
        "        before = len(existing)\n",
        "        # ðŸ§¹ drop old rows for this df_name (so we don't duplicate)\n",
        "        existing = existing[existing[\"df_name\"] != provided_name]\n",
        "        removed = before - len(existing)\n",
        "        print(f\"ðŸ§¹ Removed {removed} old rows for df_name = {provided_name}\")\n",
        "    else:\n",
        "        print(\"'df_name' column not found in existing master, not dropping old rows.\")\n",
        "\n",
        "    standardized_df = standardized_df.reindex(columns=existing.columns)\n",
        "\n",
        "    combined = pd.concat([existing, standardized_df], ignore_index=True)\n",
        "    print(f\"Existing master had {before} rows, now {len(combined)} rows.\")\n",
        "else:\n",
        "    combined = standardized_df.copy()\n",
        "    print(\"No existing master found, creating a new one.\")\n",
        "\n",
        "combined.to_csv(MASTER_OUTPUT, index=False)\n",
        "print(f\"Master file updated: {MASTER_OUTPUT} (now {len(combined)} rows)\")"
      ],
      "metadata": {
        "id": "VXPtPayp_4RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_this = combined[combined[\"df_name\"] == provided_name].copy()\n",
        "\n",
        "df_raw_reset = df_raw.reset_index(drop=True)\n",
        "std_this = std_this.reset_index(drop=True)\n",
        "\n",
        "print(\"\\nRaw rows:\", len(df_raw_reset))\n",
        "print(\"Std rows for this df_name:\", len(std_this))\n",
        "\n",
        "if len(df_raw_reset) != len(std_this):\n",
        "    raise ValueError(\"Row count mismatch between raw and standardised subset!\")\n",
        "else:\n",
        "    print(\"Row counts match for this file. Running random consistency check...\")\n",
        "\n",
        "random_consistency_check(df_raw_reset, std_this, n_samples=25)"
      ],
      "metadata": {
        "id": "YJoF4RnvBBKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- DELETE RAW DATA FILE (KEEP metadata.csv & MASTER) ----------\n",
        "if os.path.exists(data_fn):\n",
        "    os.remove(data_fn)\n",
        "    print(f\"\\nDeleted local data file: {data_fn}\")\n",
        "else:\n",
        "    print(f\"\\nTried to delete but file not found: {data_fn}\")"
      ],
      "metadata": {
        "id": "Go9lMlMS9eNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "kqJrehZ_tOwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, datetime; os.makedirs(\"/content/drive/MyDrive/covid_sim_indl\", exist_ok=True);\n",
        "shutil.copy(\"/content/standardized_master.csv\", f\"/content/drive/MyDrive/covid_sim_indl/8_standardized_master_{datetime.datetime.now():%Y%m%d_%H%M%S}.csv\")"
      ],
      "metadata": {
        "id": "qFCSz5PA3-UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### COMBINE AND THEN BREAK INTO GITHUB FRIENDLY CHUNKS"
      ],
      "metadata": {
        "id": "FdmRGNkj6KB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "src_path = \"/content/standardized_master.csv\"\n",
        "total_bytes = os.path.getsize(src_path)\n",
        "print(f\"Source size: {total_bytes / (1024*1024):.2f} MB\")\n",
        "\n",
        "df_all = pd.read_csv(src_path, low_memory=False)\n",
        "n_rows = len(df_all)\n",
        "print(f\"Total rows: {n_rows}\")\n",
        "\n",
        "target_mb = 50\n",
        "max_mb    = 90\n",
        "\n",
        "bytes_per_row = total_bytes / n_rows\n",
        "rows_per_chunk_est = int((target_mb * 1024 * 1024) / bytes_per_row)\n",
        "print(f\"Estimated rows per ~{target_mb} MB chunk: {rows_per_chunk_est}\")\n",
        "\n",
        "out_folder = \"/content/standardized_parts\"\n",
        "os.makedirs(out_folder, exist_ok=True)\n",
        "\n",
        "row_start = 0\n",
        "part = 1\n",
        "\n",
        "while row_start < n_rows:\n",
        "    rows_this_chunk = rows_per_chunk_est\n",
        "\n",
        "    while True:\n",
        "        row_end = min(row_start + rows_this_chunk, n_rows)\n",
        "        chunk = df_all.iloc[row_start:row_end]\n",
        "\n",
        "        out_path = os.path.join(out_folder, f\"standardized_master_part_{part:02d}.csv\")\n",
        "        chunk.to_csv(out_path, index=False)\n",
        "\n",
        "        size_mb = os.path.getsize(out_path) / (1024 * 1024)\n",
        "        pct = (row_end / n_rows) * 100\n",
        "\n",
        "        if size_mb <= max_mb or (row_end - row_start) <= 5000:\n",
        "            print(f\"ðŸ’¾ {os.path.basename(out_path)} | rows {row_start}-{row_end-1} \"\n",
        "                  f\"({len(chunk)}) | {size_mb:.2f} MB | {pct:.2f}% of data\")\n",
        "            break\n",
        "        else:\n",
        "            # too big â†’ delete & shrink rows, retry\n",
        "            os.remove(out_path)\n",
        "            rows_this_chunk = int(rows_this_chunk * 0.7)  # shrink by 30%\n",
        "            print(f\"Chunk too big ({size_mb:.2f} MB). \"\n",
        "                  f\"Retrying with ~{rows_this_chunk} rows...\")\n",
        "\n",
        "    row_start = row_end\n",
        "    part += 1\n",
        "\n",
        "print(\"\\nDone! All parts are in /content/standardized_parts\")"
      ],
      "metadata": {
        "id": "KRrvA-5ZORGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ld9Vg7gdVRtR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}